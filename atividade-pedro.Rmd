---
title: "Atividade ML - SVRM"
author: "Arthur Henrique Elias de Lima"
date: "2023-09-25"
output: html_document
---



## Carregando bibliotecas necessárias

```{r}
library(tidymodels)
library(tidyverse)
library(GGally)
library(skimr)

# Resolvendo possíveis conflitos entre o tidymodels e outras bibliotecas
tidymodels::tidymodels_prefer()
```

## Importando a base de dados

Base: https://www.kaggle.com/datasets/uciml/default-of-credit-card-clients-dataset/data

Informações do conjunto de dados

Este conjunto de dados contém informações sobre pagamentos inadimplentes, fatores demográficos, dados de crédito, histórico de pagamentos e extratos de contas de clientes de cartão de crédito em Taiwan de abril de 2005 a setembro de 2005.


```{r}
dados <- readr::read_csv(file = "C:/Users/arthu/Downloads/archive (2)/UCI_Credit_Card.csv")
dados<- dplyr::rename(dados, df_pay=default.payment.next.month) 
dados <- dados[-1,]
```

## Uma exploração rápida dos dados

Vamos observar o time de dados que temos

```{r}
visdat::vis_dat(dados)
```

O gráfico acima mostra que temos uma base de dados onde todas as *features* presentes na base são numéricas.

Como vamos praticar o metodo KNN, aleatoriamente será selecionado objetos para se transformar em NA

```{r}
criar_na_aleatoriamente <- function(dados, proporcao_na = 0.10) {
  dados1 <- dados
  
  num_colunas <- sample(1:ncol(dados1), 1)
  
  num_obs_com_na <- floor(proporcao_na * nrow(dados1))
  
  colunas_com_na <- sample(1:ncol(dados1), num_colunas)
  
  indices_com_na <- sample(1:nrow(dados1), num_obs_com_na)
  
  for (coluna in colunas_com_na) {
    dados1[indices_com_na, coluna] <- NA
  }
  
  return(dados1)
}

set.seed(123)  

dados1 <- criar_na_aleatoriamente(dados)

```

Um resumo dos dados poderá ser obtido utilizando a função `glimpse` do
pacote **dplyr** que é carregado com a biblioteca **tidyverse** de R.

```{r}
dados1 |> 
  dplyr::glimpse()
```

É possível todas as correlações entre todas as variáveis da base, com a
função `data_vis_cor`. Um gráfico útil com as correlações poderá ser
obtido usando a função `vis_cor`, conforme abaixo:

```{r}
visdat::data_vis_cor(dados1)
visdat::vis_cor(dados1)
```



As bibliotecas **GGally** e **skimr** também possuem funções úteis que
podem nos auxiliar no processo de exploração dos dados.

```{r}
dados1 |> 
  GGally::ggpairs()

dados1 |> 
  skimr::skim()
```

## Construindo os workflows dos modelos

Iremos comparar os modelos de regressão linar utilizando *elastic net*,
com o método $k$NN, *suport vector regression machine* - SVRM, k-folds cross-validation utilizando data splitting, Leave-one-out cross-validation.


### Divisão dos dados

Faremos uma divisão de 80% para o conjunto de treino e 20% para o conjunto de teste e iremos estratificar pela label df_pay. Ou seja, queremos que nossa divisão esteja o mais bem representado possível.

```{r}

divisao_inicial <- rsample::initial_split(dados1, prop = 0.8, strata = "df_pay")
treinamento <- rsample::training(divisao_inicial) # Conjunto de treinamento
teste <- rsample::testing(divisao_inicial) # Conjunto de teste
```

### Tratamento dos dados (pré-processamento)

```{r}

receita <-  treinamento %>%  recipe(formula = df_pay~. , dados1) %>% 
  step_impute_knn(all_predictors(),impute_with =c("PAY_0","PAY_2","PAY_3", "PAY_4", "PAY_5","PAY_6"), neighbors = 10) %>% 
  step_YeoJohnson(all_predictors()) %>% 
  step_normalize(all_predictors()) %>% 
  step_corr(all_predictors())

receita_1 <- 
  treinamento |> 
    recipe(formula = df_pay ~ .) |>
    step_YeoJohnson(all_predictors()) |>
    step_normalize(all_predictors()) |>
    step_zv(all_predictors()) |>
    step_corr(all_predictors())

receita_2 <- 
  treinamento |> 
    recipe(formula = df_pay ~ .) |>
    step_YeoJohnson(all_predictors()) |>
    step_normalize(all_predictors())
```


verificação da receita

```{r}
receita |> 
  prep() |> 
  juice()
```
Usamos KNN primeiro, assim retiramos os NA com os 10 vizinhos mais proximos

### Definindo os modelos

O código que segue faz a configuração realiza a configuração dos modelos
que serão comparados. O código `tune::tune()` especifica que o
respectivo parâmetro de sintonização será obtido no processo de
validação cruzada, particularmente, um *grid search*.

```{r}
modelo_elastic <- 
  linear_reg(penalty = tune(), mixture = tune()) |> 
  set_mode("regression") |> 
  set_engine("glmnet")

modelo_knn <- nearest_neighbor(
  neighbors = tune(),
  dist_power = tune(), 
  weight_func = "gaussian" 
) %>%
  set_mode("regression") %>%
  set_engine("kknn")

modelo_svm <- 
  svm_rbf(
    cost = tune(),
    rbf_sigma = tune(),
    margin = tune()
  ) |> 
  set_mode("regression") |> 
  set_engine("kernlab")

wf_knn <-
  workflow() %>%
  add_recipe(receita) %>%
  add_model(modelo_knn)

wf_elastic <- 
  workflow() %>% 
  add_recipe(receita) %>% 
  add_model(modelo_elastic)

wf_svm <-
  workflow() %>%
  add_recipe(receita) %>%
  add_model(modelo_svm)

grid_control <- control_grid(
  save_pred = TRUE,
  save_workflow = TRUE,
  parallel_over = "resamples"
)

vfold <- 
  treinamento |> 
  vfold_cv(v = 10, strata = df_pay)

metrica <- metric_set(rmse)
```

### Criando o conjunto de validação 


Utilizaremos $k=8$:

```{r}
validacao_cruzada <- 
  treinamento |> 
  rsample::vfold_cv(v = 8L, strata = df_pay)
```

### Criando um *workflow* completo 

Aqui criaremos um *workflow* completo com todos modelos a serem comparados. Chamaremos ele de `wf_todos`:

```{r}
wf_todos <-
  workflow_set(
    preproc = list(receita),
    models = list(
      knn_fit = modelo_knn,
      elastic_fit = modelo_elastic,
      svm_fit = modelo_svm
      
    ),
    cross = TRUE
  )
```


Assim, criaremos o objeto `controle_grid`, para que possamos passar a função `workflow_map` posteriormente. Tem-se:

```{r}
controle_grid <- control_grid(
  save_pred = TRUE,
  save_workflow = TRUE,
  parallel_over = "resamples"
)
```

### Trainando o modelo



```{r}
treino <-
  wf_todos |> 
  workflow_map(
    resamples = validacao_cruzada,
    grid = 10L,
    control = controle_grid
  )
```


```{r}

grid_knn <- grid_max_entropy(wf_knn %>% extract_parameter_set_dials(), size = 50L)
grid_elastic <- grid_max_entropy(wf_elastic %>% extract_parameter_set_dials(), size = 50L)
grid_svm <-  grid_max_entropy(wf_svm %>% extract_parameter_set_dials(), size = 50L)
```

fazendo separado
```{r}
tune_knn <- 
  tune_grid(wf_knn,
            grid = grid_knn,
            resamples = vfold,
            metrics = metrica,
            control = grid_control,
            )

tune_elastic <- 
  tune_grid(wf_elastic,
            grid = grid_elastic,
            resamples = vfold,
            metrics = metrica,
            control = grid_control)

tune_svm <- 
  tune_grid(wf_svm,
            grid = grid_svm,
            resamples = vfold,
            metrics = metrica,
            control = grid_control)


wf_svm <- 
  wf_svm |> 
  finalize_workflow(select_best(tune_svm))

wf_knn <- 
  wf_knn %>% 
  finalize_workflow(select_best(tune_knn))

wf_elastic <- 
  wf_elastic %>% 
  finalize_workflow(select_best(tune_elastic))

teste_knn <- 
  wf_knn %>% 
  last_fit(split = splitted)

teste_elastic <- 
  wf_elastic %>% 
  last_fit(split = splitted)

teste_svm <- 
  wf_svm %>% 
  last_fit(split = splitted)
```


A função `autoplot`
 do pacote **ggplot2** é útil para que possamos visualizar o desempenho 
de cada um dos modelos considerando a métrica do EQM. Isso é feito da 
seguinte forma:

```{r}
autoplot(treino, metric = "rmse") + 
  labs(
    title = "Avaliação dos modelos de regressão",
    subtitle = "Utilizando a métrica do EQM"
  ) + 
  xlab("Rank dos Workflows") +
  ylab("Erro Quadrático Médio - EQM")
```
Perceba
 que no gráfico acima, temos os 8 modelos avaliados no $8$-*folds 
cross-validation*. Portanto, para cada um dos modelos comparados, temos 8
 avaliações. Caso deseje avaliar as métricas do melhor cenário de cada 
um dos modelos, fazemos:

```{r}
melhores <- 
  treino |> 
  rank_results(select_best = TRUE, rank_metric = "rmse")

autoplot(treino, select_best = TRUE)

```
Vamos
 agora selecionar o melhor modelo dentre os modelos comparados. Isso não
 quer dizer que o modelo seja bom para resolver o problema em questão. 
Para ver um rank e saber qual modelo e receita foram as melhores, 
fazemos:

```{r}
treino |> 
  rank_results()
```

Assim, podemos perceber que a `receita_1` combinada com o modelo $k$NN é o melhor escolha entre os modelos e receitas comparadas. Portanto:

```{r}
melhor_modelo <- 
  treino |> 
  extract_workflow_set_result(id = "recipe_1_knn_fit") |> 
  select_best(metric = "rmse")

melhor_modelo
```

### Avaliação final do melhor modelo

Após a escolha do melhor modelo e da estimação de seus hiperparâmetros, nesse caso, o modelo $k$NN com $k = 12$ e `dist_power \approx 1.47`, precisamos testar o desempenho do modelo final segundo a base de dados de teste. Para tanto, utilizamos a função `last_fit` do pacote [tune](https://tune.tidymodels.org/reference/last_fit.html). Temos que:

```{r}
wf_final <- 
  treino |> 
  extract_workflow(id = "recipe_1_knn_fit") |> 
  finalize_workflow(melhor_modelo)

teste <- 
  wf_final |> 
  last_fit(split = divisao_inicial)

teste$.metrics
```

Agora
 que temos os hiperparâmetros estimados e temos uma boa estimativa do 
risco preditivo real do modelo final selecionado, poderemos preceder com
 um ajuste final, com toda a base de dados (treinamento + teste).

```{r}
modelo_final <- 
  wf_final |> 
  fit(dados)
```

### Salvando o modelo

Depois
 que temos o modelo finalizado, podemos ter alguns interesses de como 
utilizar o resultado, i.e., o modelo treinado. Entre alguns motivos, 
posso citar:

1. Salvar o modelo para uso no futuro, sem ter que retreinar;
2. Distribuir o modelo para que outras pessoas possam experimentar, sem terem que executar seu script R e retreinar o modelo;
3. Introduzir seu modelo treinado em uma API que irá consumir os resultados, i.e., consumir as previsões do modelo.

Nessas
 situações, é conveniente salvar o modelo em um arquivo serializado (*R 
Data Serialization*). Tais arquivos possuem a extensão `.rds`. Devemos fazer:


```{r}
#| eval: false

# Salvando o modelo em um arquivo serializado
saveRDS(modelo_final, file = "modelo_final.rds")

# Lendo o arquivo serializado com o modelo final
load(file = "modelo_final.rds")

# Fazendo novas previsões
predict(modelo_final, new_data = novos_dados)
```